{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 10.0.19045.4291]\n",
      "(c) Microsoft Corporation. All rights reserved.\n",
      "\n",
      "(ydata) g:\\Codes 2024\\Capstone_Final\\reffrence_code>%%capture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'%%capture' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(ydata) g:\\Codes 2024\\Capstone_Final\\reffrence_code>pip install nltk\n",
      "Requirement already satisfied: nltk in g:\\codes 2024\\capstone_final\\ydata\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in g:\\codes 2024\\capstone_final\\ydata\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in g:\\codes 2024\\capstone_final\\ydata\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in g:\\codes 2024\\capstone_final\\ydata\\lib\\site-packages (from nltk) (2024.4.28)\n",
      "Requirement already satisfied: tqdm in g:\\codes 2024\\capstone_final\\ydata\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in g:\\codes 2024\\capstone_final\\ydata\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "\n",
      "(ydata) g:\\Codes 2024\\Capstone_Final\\reffrence_code>pip install scikit-learn\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.4.2-cp39-cp39-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in g:\\codes 2024\\capstone_final\\ydata\\lib\\site-packages (from scikit-learn) (1.22.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in g:\\codes 2024\\capstone_final\\ydata\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in g:\\codes 2024\\capstone_final\\ydata\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.4.2-cp39-cp39-win_amd64.whl (10.6 MB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "Successfully installed scikit-learn-1.4.2 threadpoolctl-3.5.0\n",
      "\n",
      "(ydata) g:\\Codes 2024\\Capstone_Final\\reffrence_code>"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "%%capture\n",
    "pip install nltk\n",
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example DataFrame containing questions and their corresponding answers\n",
    "df = pd.DataFrame({\n",
    "    'question': [\"What is the capital of France?\", \"Who wrote 'Romeo and Juliet'?\"],\n",
    "    'expected_answer': [\"Paris\", \"William Shakespeare\"],\n",
    "    'generated_answer': [\"Paris\", \"William Shakespeare\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(row):\n",
    "    return row['expected_answer'] == row['generated_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy    :  1.0\n",
      "Precision   :  1.0\n",
      "Recall      :  1.0\n",
      "F1-score    :  1.0\n",
      "BLEU score  :  1.254338396921439e-154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Codes 2024\\Capstone_Final\\ydata\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "g:\\Codes 2024\\Capstone_Final\\ydata\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to each row\n",
    "df['correct'] = df.apply(calculate_accuracy, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = df['correct'].mean()\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(\n",
    "    df['expected_answer'], \n",
    "    df['generated_answer'], \n",
    "    average='weighted'\n",
    "    )\n",
    "\n",
    "recall = recall_score(\n",
    "    df['expected_answer'], \n",
    "    df['generated_answer'], \n",
    "    average='weighted'\n",
    "    )\n",
    "\n",
    "f1 = f1_score(\n",
    "    df['expected_answer'], \n",
    "    df['generated_answer'], \n",
    "    average='weighted')\n",
    "\n",
    "# Calculate BLEU score\n",
    "references = [[a.split()] for a in df['expected_answer']]\n",
    "hypotheses = [a.split() for a in df['generated_answer']]\n",
    "bleu_score = corpus_bleu(references, hypotheses)\n",
    "\n",
    "print(\"Accuracy    : \", accuracy)\n",
    "print(\"Precision   : \", precision)\n",
    "print(\"Recall      : \", recall)\n",
    "print(\"F1-score    : \", f1)\n",
    "print(\"BLEU score  : \", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
